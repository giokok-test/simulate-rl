# Training related options
training:
  # Number of training episodes
  episodes: 300000
  # Optimiser learning rate
  learning_rate: 0.05
  # L2 weight decay applied by the optimiser
  weight_decay: 0.0
  # StepLR schedule - set step size to 0 to disable
  lr_step_size: 0
  # Multiplicative factor of learning rate decay
  lr_gamma: 0.995
  # Width of hidden layers in the policy networks
  hidden_size: 64
  # Activation function: relu | tanh | leaky_relu
  activation: relu
  # Reward threshold for sample efficiency metric
  reward_threshold: 0.0
  # Run evaluation episodes every this many training episodes
  eval_freq: 20000
  # Episodes per bin for termination statistics
  outcome_window: 100
  # Save a checkpoint every this many episodes. Set to 0 to disable.
  checkpoint_steps: 2000
  # Curriculum progression strategy: "linear" interpolates from the
  # starting configuration to the final one over a fixed number of
  # stages while "adaptive" advances only when the recent success
  # rate exceeds ``success_threshold``.
  curriculum_mode: adaptive
  # Minimum fraction of successful episodes required to move to the
  # next curriculum stage when ``curriculum_mode`` is "adaptive".
  success_threshold: 0.6
  # Window size used to compute the adaptive success rate.
  curriculum_window: 256
  # Number of discrete curriculum stages including the final environment.
  # If set to ``N`` there will be ``N - 1`` transitions from ``curriculum.start``
  # to ``curriculum.end`` over the course of training.
  curriculum_stages: 20
  # PPO algorithm parameters
  gamma: 0.99
  clip_ratio: 0.2
  ppo_epochs: 4
  entropy_coef_start: 0.01
  entropy_coef_end: 0.01
  # Curriculum specifying how the starting conditions gradually expand
  # throughout training. The ``start`` dictionary defines the easiest
  # configuration while ``end`` corresponds to the final environment
  # parameters. All numeric values are interpolated logarithmically when
  # positive, falling back to linear interpolation otherwise.
  curriculum:
    start:
      evader_start:
        distance_range: [10000.0, 10000.0]
        initial_speed: 1.0
      pursuer_start:
        cone_half_angle: 1.5708
        inner_cone_half_angle: 1.5708
        min_range: 10.0
        max_range: 50.0
        yaw_range: [0.0, 0.0]
        # Initial pursuer speed sampled from this range [m/s]
        initial_speed_range: [50.0, 50.0]
        force_target_radius: 0.0
    end:
      evader_start:
        distance_range: [8000.0, 12000.0]
        initial_speed: 50
      pursuer_start:
        cone_half_angle: 1.5
        inner_cone_half_angle: 1.3
        min_range: 1000.0
        max_range: 5000.0
        yaw_range: [-2, 2]
        # Final range of pursuer spawn speeds [m/s]
        initial_speed_range: [50.0, 75.0]
        force_target_radius: 350.0
