evader:
  # Mass of the evader aircraft [kg]
  mass: 1500.0
  # Maximum acceleration magnitude the evader can command [m/s^2]
  max_acceleration: 15.0
  # Upper speed limit [m/s]
  top_speed: 100.0
  # Dimensionless coefficient modelling drag
  drag_coefficient: 0
  # How much information the evader gets about the pursuer
  #   1 - evader has no knowledge of the pursuer
  #   2 - only the distance to the pursuer is observed
  #   3 - evader receives the unit vector pointing to the pursuer
  #   4 - full pursuer position is known (values >4 behave the same)
  awareness_mode: 1
  # Maximum yaw rotation rate [deg/s]
  yaw_rate: 5
  # Maximum pitch rotation rate [deg/s]
  pitch_rate: 10
  # Initial force direction
  up_vector: [0.0, 0.0, 1.0]
  # Maximum allowable pitch angle [deg]
  stall_angle: 45.0
  # Preset trajectory mode. Use "dive" to level off until the dive angle
  # threshold is reached
  trajectory: direct
  # Angle to the target at which the evader begins its dive [deg]
  dive_angle: 20.0
pursuer:
  # Mass of the pursuer [kg]
  mass: 100.0
  # Maximum acceleration magnitude [m/s^2]
  max_acceleration: 30.0
  # Commanded acceleration magnitude [non-negative]
  # Upper speed limit [m/s]
  top_speed: 150.0
  # Dimensionless coefficient modelling drag
  drag_coefficient: 0.0
  # Maximum yaw rotation rate [deg/s]
  yaw_rate: 20.0
  # Maximum pitch rotation rate [deg/s]
  pitch_rate: 25.0
  # Initial force direction
  up_vector: [0.0, 0.0, 1.0]
  # Maximum allowable pitch angle [deg]
  stall_angle: 55.0
# Downward gravitational acceleration [m/s^2]
gravity: 0
# Simulation time step [s]
time_step: 0.1
# Duration of one episode [min]
episode_duration: 40
# Distance at which capture is considered successful [m]
capture_radius: 1.0
# Distance within which the evader receives reward when hitting the ground [m]
target_reward_distance: 100.0
# Weight for shaping rewards
shaping_weight: 0.05
# Additional reward weight for simply getting closer to the evader each step
closer_weight: 0.05
# Additional reward weight for reducing the pursuer-to-evader angle
angle_weight: 0.0
# Coordinates of the evader's goal [m]
target_position: [0, 0.0, 0.0]
evader_start:
  # Range of initial horizontal distance from the goal [m]
  distance_range: [8000.0, 12000.0]
  # Starting altitude [m]
  altitude: 3000.0
  # Initial speed directed toward the target [m/s]
  initial_speed: 50.0
pursuer_start:
  # Maximum half angle of the pursuer spawn cone [rad]
  cone_half_angle: 1.5
  # Minimum half angle of the pursuer spawn cone [rad]
  inner_cone_half_angle: 1.3
  # Horizontal angular range relative to directly behind the evader [rad]
  # When set this overrides the ``sections`` breakdown into quadrants.
  yaw_range: [-3.14159, 3.14159]
  # Minimum initial range from the evader [m]
  min_range: 1000.0
  # Maximum initial range from the evader [m]
  max_range: 5000.0
  # Radius around the evader target where the pursuer aims initially [m]
  force_target_radius: 150.0
  # Range of random initial speeds [m/s]
  initial_speed_range: [50.0, 75.0]
# Percentage error applied to angular measurements of the opposing agent
measurement_error_pct: 0.0

# Factor multiplying the starting distance between the agents that
# triggers episode termination when their separation exceeds this
# multiple.
separation_cutoff_factor: 2.0

# Training related options
training:
  # Number of training episodes
  episodes: 5000
  # Optimiser learning rate
  learning_rate: 0.001
  # L2 weight decay applied by the optimiser
  weight_decay: 0.0
  # StepLR schedule - set step size to 0 to disable
  lr_step_size: 0
  # Multiplicative factor of learning rate decay
  lr_gamma: 0.95
  # Width of hidden layers in the policy networks
  hidden_size: 64
  # Activation function: relu | tanh | leaky_relu
  activation: relu
  # Reward threshold for sample efficiency metric
  reward_threshold: 0.0
  # Run evaluation episodes every this many training episodes
  eval_freq: 1000
  # Save a checkpoint every this many episodes. Set to 0 to disable.
  checkpoint_steps: 0
  # Curriculum progression mode: linear | adaptive
  curriculum_mode: linear
  # Rolling success threshold for adaptive curriculum
  curriculum_success_threshold: 0.8
  # Number of evaluation results to average for adaptive curriculum
  curriculum_window: 5
  # Number of discrete curriculum stages
  curriculum_stages: 5
  # Curriculum specifying how the starting conditions gradually expand
  # throughout training. The ``start`` dictionary defines the easiest
  # configuration while ``end`` corresponds to the final environment
  # parameters. All numeric values are linearly interpolated from the
  # beginning to the end of training.
  curriculum:
    start:
      evader_start:
        distance_range: [10000.0, 10000.0]
      pursuer_start:
        cone_half_angle: 0.0
        inner_cone_half_angle: 0.0
        min_range: 1000.0
        max_range: 1000.0
        yaw_range: [0.0, 0.0]
        force_target_radius: 0.0
    end:
      evader_start:
        distance_range: [8000.0, 12000.0]
      pursuer_start:
        cone_half_angle: 1.5
        inner_cone_half_angle: 1.3
        min_range: 1000.0
        max_range: 5000.0
        yaw_range: [-3.14159, 3.14159]
        force_target_radius: 150.0
