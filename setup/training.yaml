# Q-learning training options
q_learning:
  episodes: 100000
  lr: 0.001
  gamma: 0.99
  epsilon_start: 1.0
  epsilon_end: 0.05
  epsilon_decay: 0.995
  batch_size: 128
  buffer_size: 50000
  target_update: 200
  eval_freq: 50000
  max_steps: 5000
  log_dir: null
  capture_bonus: 0.0
  checkpoint_every: 5000

curriculum:
  mode: adaptive          # or "fixed" for linear progression
  success_threshold: 0.6  # required capture rate for adaptive mode
  window: 128              # episodes considered for success rate
  stages: 120             # number of discrete curriculum stages
  start:
    evader_start:
      distance_range: [10000.0, 10000.0]
      initial_speed: 1.0
    pursuer_start:
      cone_half_angle: 1.5708
      inner_cone_half_angle: 1.5708
      min_range: 10.0
      max_range: 50.0
      yaw_range: [0.0, 0.0]
      initial_speed_range: [50.0, 50.0]
      force_target_radius: 0.0
  end:
    evader_start:
      distance_range: [8000.0, 12000.0]
      initial_speed: 50.0
    pursuer_start:
      cone_half_angle: 1.5
      inner_cone_half_angle: 1.3
      min_range: 1000.0
      max_range: 5000.0
      yaw_range: [-2.0, 2.0]
      initial_speed_range: [50.0, 75.0]
      force_target_radius: 350.0

